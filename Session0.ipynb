{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from fastNLP.io import Pipe, ConllLoader\n",
    "from fastNLP.io import DataBundle\n",
    "from fastNLP.io.pipe.utils import _add_words_field, _indexize\n",
    "from fastNLP.io.pipe.utils import iob2, iob2bioes\n",
    "from fastNLP.io.pipe.utils import _add_chars_field\n",
    "from fastNLP.io.utils import check_loader_paths\n",
    "\n",
    "from fastNLP.io import Conll2003NERLoader\n",
    "from fastNLP import Const\n",
    "\n",
    "def word_shape(words):\n",
    "    shapes = []\n",
    "    for word in words:\n",
    "        caps = []\n",
    "        for char in word:\n",
    "            caps.append(char.isupper())\n",
    "        if all(caps):\n",
    "            shapes.append(0)\n",
    "        elif any(caps) is False:\n",
    "            shapes.append(1)\n",
    "        elif caps[0]:\n",
    "            shapes.append(2)\n",
    "        elif any(caps):\n",
    "            shapes.append(3)\n",
    "        else:\n",
    "            shapes.append(4)\n",
    "    return shapes\n",
    "\n",
    "\n",
    "class Conll2003NERPipe(Pipe):\n",
    "    \"\"\"\n",
    "    Conll2003的NER任务的处理Pipe, 该Pipe会（1）复制raw_words列，并命名为words; (2）在words, target列建立词表\n",
    "    (创建 :class:`fastNLP.Vocabulary` 对象，所以在返回的DataBundle中将有两个Vocabulary); (3）将words，target列根据相应的\n",
    "    Vocabulary转换为index。\n",
    "    经过该Pipe过后，DataSet中的内容如下所示\n",
    "\n",
    "    .. csv-table:: Following is a demo layout of DataSet returned by Conll2003Loader\n",
    "       :header: \"raw_words\", \"target\", \"words\", \"seq_len\"\n",
    "\n",
    "       \"[Nadim, Ladki]\", \"[1, 2]\", \"[2, 3]\", 2\n",
    "       \"[AL-AIN, United, Arab, ...]\", \"[3, 4,...]\", \"[4, 5, 6,...]\", 6\n",
    "       \"[...]\", \"[...]\", \"[...]\", .\n",
    "\n",
    "    raw_words列为List[str], 是未转换的原始数据; words列为List[int]，是转换为index的输入数据; target列是List[int]，是转换为index的\n",
    "    target。返回的DataSet中被设置为input有words, target, seq_len; 设置为target有target。\n",
    "\n",
    "    dataset的print_field_meta()函数输出的各个field的被设置成input和target的情况为::\n",
    "\n",
    "        +-------------+-----------+--------+-------+---------+\n",
    "        | field_names | raw_words | target | words | seq_len |\n",
    "        +-------------+-----------+--------+-------+---------+\n",
    "        |   is_input  |   False   |  True  |  True |   True  |\n",
    "        |  is_target  |   False   |  True  | False |   True  |\n",
    "        | ignore_type |           | False  | False |  False  |\n",
    "        |  pad_value  |           |   0    |   0   |    0    |\n",
    "        +-------------+-----------+--------+-------+---------+\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding_type: str = 'bio', lower: bool = False, word_shape: bool=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param: str encoding_type: target列使用什么类型的encoding方式，支持bioes, bio两种。\n",
    "        :param bool lower: 是否将words小写化后再建立词表，绝大多数情况都不需要设置为True。\n",
    "        :param boll word_shape: 是否新增一列word shape，5维\n",
    "        \"\"\"\n",
    "        if encoding_type == 'bio':\n",
    "            self.convert_tag = iob2\n",
    "        elif encoding_type == 'bioes':\n",
    "            self.convert_tag = lambda words: iob2bioes(iob2(words))\n",
    "        else:\n",
    "            raise ValueError(\"encoding_type only supports `bio` and `bioes`.\")\n",
    "        self.lower = lower\n",
    "        self.word_shape = word_shape\n",
    "\n",
    "    def process(self, data_bundle: DataBundle) -> DataBundle:\n",
    "        \"\"\"\n",
    "        支持的DataSet的field为\n",
    "\n",
    "        .. csv-table::\n",
    "           :header: \"raw_words\", \"target\"\n",
    "\n",
    "           \"[Nadim, Ladki]\", \"[B-PER, I-PER]\"\n",
    "           \"[AL-AIN, United, Arab, ...]\", \"[B-LOC, B-LOC, I-LOC, ...]\"\n",
    "           \"[...]\", \"[...]\"\n",
    "\n",
    "        :param ~fastNLP.DataBundle data_bundle: 传入的DataBundle中的DataSet必须包含raw_words和ner两个field，且两个field的内容均为List[str]在传入DataBundle基础上原位修改。\n",
    "        :return DataBundle:\n",
    "        \"\"\"\n",
    "        # 转换tag\n",
    "        for name, dataset in data_bundle.datasets.items():\n",
    "            dataset.apply_field(self.convert_tag, field_name=Const.TARGET, new_field_name=Const.TARGET)\n",
    "\n",
    "        _add_words_field(data_bundle, lower=self.lower)\n",
    "\n",
    "        if self.word_shape:\n",
    "            data_bundle.apply_field(word_shape, field_name='raw_words', new_field_name='word_shapes')\n",
    "            data_bundle.set_input('word_shapes')\n",
    "\n",
    "        # 将所有digit转为0\n",
    "        data_bundle.apply_field(lambda chars:[''.join(['0' if c.isdigit() else c for c in char]) for char in chars],\n",
    "                field_name=Const.INPUT, new_field_name=Const.INPUT)\n",
    "\n",
    "        # index\n",
    "        _indexize(data_bundle)\n",
    "\n",
    "        input_fields = [Const.TARGET, Const.INPUT, Const.INPUT_LEN]\n",
    "        target_fields = [Const.TARGET, Const.INPUT_LEN]\n",
    "\n",
    "        for name, dataset in data_bundle.datasets.items():\n",
    "            dataset.add_seq_len(Const.INPUT)\n",
    "\n",
    "        data_bundle.set_input(*input_fields)\n",
    "        data_bundle.set_target(*target_fields)\n",
    "\n",
    "        return data_bundle\n",
    "\n",
    "    def process_from_file(self, paths) -> DataBundle:\n",
    "        \"\"\"\n",
    "\n",
    "        :param paths: 支持路径类型参见 :class:`fastNLP.io.loader.ConllLoader` 的load函数。\n",
    "        :return: DataBundle\n",
    "        \"\"\"\n",
    "        # 读取数据\n",
    "        data_bundle = Conll2003NERLoader().load(paths)\n",
    "        data_bundle = self.process(data_bundle)\n",
    "\n",
    "        return data_bundle\n",
    "\n",
    "\n",
    "def bmeso2bio(tags):\n",
    "    new_tags = []\n",
    "    for tag in tags:\n",
    "        tag = tag.lower()\n",
    "        if tag.startswith('m') or tag.startswith('e'):\n",
    "            tag = 'i' + tag[1:]\n",
    "        if tag.startswith('s'):\n",
    "            tag = 'b' + tag[1:]\n",
    "        new_tags.append(tag)\n",
    "    return new_tags\n",
    "\n",
    "\n",
    "def bmeso2bioes(tags):\n",
    "    new_tags = []\n",
    "    for tag in tags:\n",
    "        lowered_tag = tag.lower()\n",
    "        if lowered_tag.startswith('m'):\n",
    "            tag = 'i' + tag[1:]\n",
    "        new_tags.append(tag)\n",
    "    return new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/eng.train.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = Conll2003NERPipe().process_from_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastNLP.io.data_bundle.DataBundle"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNERPipe(Pipe):\n",
    "    def __init__(self, bigrams=False, encoding_type='bmeso'):\n",
    "        super().__init__()\n",
    "        self.bigrams = bigrams\n",
    "        if encoding_type=='bmeso':\n",
    "            self.encoding_func = lambda x:x\n",
    "        elif encoding_type=='bio':\n",
    "            self.encoding_func = bmeso2bio\n",
    "        elif encoding_type == 'bioes':\n",
    "            self.encoding_func = bmeso2bioes\n",
    "        else:\n",
    "            raise RuntimeError(\"Only support bio, bmeso, bioes\")\n",
    "\n",
    "    def process(self, data_bundle: DataBundle):\n",
    "        _add_chars_field(data_bundle, lower=False)\n",
    "\n",
    "        data_bundle.apply_field(self.encoding_func, field_name=Const.TARGET, new_field_name=Const.TARGET)\n",
    "\n",
    "        # 将所有digit转为0\n",
    "        data_bundle.apply_field(lambda chars:[''.join(['0' if c.isdigit() else c for c in char]) for char in chars],\n",
    "            field_name=Const.CHAR_INPUT, new_field_name=Const.CHAR_INPUT)\n",
    "\n",
    "        #\n",
    "        input_field_names = [Const.CHAR_INPUT]\n",
    "        if self.bigrams:\n",
    "            data_bundle.apply_field(lambda chars:[c1+c2 for c1,c2 in zip(chars, chars[1:]+['<eos>'])],\n",
    "                                    field_name=Const.CHAR_INPUT, new_field_name='bigrams')\n",
    "            input_field_names.append('bigrams')\n",
    "\n",
    "        # index\n",
    "        _indexize(data_bundle, input_field_names=input_field_names, target_field_names=Const.TARGET)\n",
    "\n",
    "        input_fields = [Const.TARGET, Const.INPUT_LEN] + input_field_names\n",
    "        target_fields = [Const.TARGET, Const.INPUT_LEN]\n",
    "\n",
    "        for name, dataset in data_bundle.datasets.items():\n",
    "            dataset.add_seq_len(Const.CHAR_INPUT)\n",
    "\n",
    "        data_bundle.set_input(*input_fields)\n",
    "        data_bundle.set_target(*target_fields)\n",
    "\n",
    "        return data_bundle\n",
    "\n",
    "    def process_from_file(self, paths):\n",
    "        paths = check_loader_paths(paths)\n",
    "        loader = ConllLoader(headers=['raw_chars', 'target'])\n",
    "        data_bundle = loader.load(paths)\n",
    "        return self.process(data_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2  = CNNERPipe().process_from_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': +---------------------+---------------------+---------------------+---------+\n",
       " | raw_words           | target              | words               | seq_len |\n",
       " +---------------------+---------------------+---------------------+---------+\n",
       " | ['EU', 'rejects'... | [3, 0, 6, 0, 0, ... | [902, 10924, 221... | 9       |\n",
       " | ['Peter', 'Black... | [2, 4]              | [683, 1975]         | 2       |\n",
       " | ['BRUSSELS', '19... | [1, 0]              | [1306, 25]          | 2       |\n",
       " | ['The', 'Europea... | [0, 3, 5, 0, 0, ... | [20, 215, 423, 1... | 30      |\n",
       " | ['Germany', \"'s\"... | [1, 0, 0, 0, 0, ... | [118, 17, 2987, ... | 31      |\n",
       " | ['\"', 'We', 'do'... | [0, 0, 0, 0, 0, ... | [14, 121, 165, 1... | 33      |\n",
       " | ['He', 'said', '... | [0, 0, 0, 0, 0, ... | [90, 16, 662, 23... | 25      |\n",
       " | ['He', 'said', '... | [0, 0, 0, 0, 0, ... | [90, 16, 10, 215... | 40      |\n",
       " | ['Fischler', 'pr... | [2, 0, 6, 0, 0, ... | [3400, 1488, 109... | 28      |\n",
       " | ['But', 'Fischle... | [0, 2, 0, 0, 0, ... | [111, 3400, 380,... | 37      |\n",
       " | ['Spanish', 'Far... | [6, 0, 0, 2, 4, ... | [1706, 5851, 146... | 27      |\n",
       " | ['.']               | [0]                 | [2]                 | 1       |\n",
       " | ['Only', 'France... | [0, 1, 0, 1, 0, ... | [3971, 135, 13, ... | 9       |\n",
       " | ['The', 'EU', \"'... | [0, 3, 0, 0, 0, ... | [20, 902, 17, 23... | 26      |\n",
       " | ['Sheep', 'have'... | [0, 0, 0, 0, 0, ... | [7620, 44, 605, ... | 30      |\n",
       " | ['British', 'far... | [6, 0, 0, 0, 0, ... | [199, 1155, 904,... | 35      |\n",
       " | ['\"', 'What', 'w... | [0, 0, 0, 0, 0, ... | [14, 1834, 104, ... | 39      |\n",
       " | ['Bonn', 'has', ... | [1, 0, 0, 0, 0, ... | [2644, 37, 562, ... | 34      |\n",
       " | ['Germany', 'imp... | [1, 0, 0, 0, 0, ... | [118, 3973, 87, ... | 15      |\n",
       " | ['It', 'brought'... | [0, 0, 0, 0, 0, ... | [91, 685, 8, 81,... | 16      |\n",
       " | ['Rare', 'Hendri... | [0, 2, 0, 0, 0, ... | [10939, 3974, 76... | 10      |\n",
       " | ['LONDON', '1996... | [1, 0]              | [200, 25]           | 2       |\n",
       " | ['A', 'rare', 'e... | [0, 0, 0, 0, 0, ... | [92, 2646, 254, ... | 35      |\n",
       " | ['A', 'Florida',... | [0, 1, 0, 0, 0, ... | [92, 1226, 2994,... | 35      |\n",
       " | ['At', 'the', 'e... | [0, 0, 0, 0, 0, ... | [226, 4, 262, 6,... | 32      |\n",
       " | ['Buyers', 'also... | [0, 0, 0, 0, 0, ... | [7629, 88, 1590,... | 30      |\n",
       " | ['They', 'includ... | [0, 0, 0, 0, 0, ... | [218, 1093, 10, ... | 33      |\n",
       " | ['The', 'guitari... | [0, 0, 0, 0, 0, ... | [20, 10952, 563,... | 12      |\n",
       " | ['China', 'says'... | [1, 0, 1, 0, 0, ... | [193, 297, 687, ... | 8       |\n",
       " | ['BEIJING', '199... | [1, 0]              | [2378, 25]          | 2       |\n",
       " | ['China', 'on', ... | [1, 0, 0, 0, 1, ... | [193, 15, 69, 54... | 35      |\n",
       " | ...                 | ...                 | ...                 | ...     |\n",
       " +---------------------+---------------------+---------------------+---------+}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from fastNLP.embeddings import TokenEmbedding\n",
    "import torch\n",
    "from fastNLP import Vocabulary\n",
    "import torch.nn.functional as F\n",
    "from fastNLP import logger\n",
    "from fastNLP.embeddings.utils import _construct_char_vocab_from_vocab, get_embeddings\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TransformerCharEmbed(TokenEmbedding):\n",
    "    def __init__(self, vocab: Vocabulary, embed_size: int = 30, char_emb_size: int = 30, word_dropout: float = 0,\n",
    "                 dropout: float = 0, pool_method: str = 'max', activation='relu',\n",
    "                 min_char_freq: int = 2, requires_grad=True, include_word_start_end=True,\n",
    "                 char_attn_type='adatrans', char_n_head=3, char_dim_ffn=60, char_scale=False, char_pos_embed=None,\n",
    "                 char_dropout=0.15, char_after_norm=False):\n",
    "        \"\"\"\n",
    "        :param vocab: 词表\n",
    "        :param embed_size: TransformerCharEmbed的输出维度。默认值为50.\n",
    "        :param char_emb_size: character的embedding的维度。默认值为50. 同时也是Transformer的d_model大小\n",
    "        :param float word_dropout: 以多大的概率将一个词替换为unk。这样既可以训练unk也是一定的regularize。\n",
    "        :param dropout: 以多大概率drop character embedding的输出以及最终的word的输出。\n",
    "        :param pool_method: 支持'max', 'avg'。\n",
    "        :param activation: 激活函数，支持'relu', 'sigmoid', 'tanh', 或者自定义函数.\n",
    "        :param min_char_freq: character的最小出现次数。默认值为2.\n",
    "        :param requires_grad:\n",
    "        :param include_word_start_end: 是否使用特殊的tag标记word的开始与结束\n",
    "        :param char_attn_type: adatrans or naive.\n",
    "        :param char_n_head: 多少个head\n",
    "        :param char_dim_ffn: transformer中ffn中间层的大小\n",
    "        :param char_scale: 是否使用scale\n",
    "        :param char_pos_embed: None, 'fix', 'sin'. What kind of position embedding. When char_attn_type=relative, None is\n",
    "            ok\n",
    "        :param char_dropout: Dropout in Transformer encoder\n",
    "        :param char_after_norm: the normalization place.\n",
    "        \"\"\"\n",
    "        super(TransformerCharEmbed, self).__init__(vocab, word_dropout=word_dropout, dropout=dropout)\n",
    "\n",
    "        assert char_emb_size%char_n_head == 0, \"d_model should divide n_head.\"\n",
    "\n",
    "        assert pool_method in ('max', 'avg')\n",
    "        self.pool_method = pool_method\n",
    "        # activation function\n",
    "        if isinstance(activation, str):\n",
    "            if activation.lower() == 'relu':\n",
    "                self.activation = F.relu\n",
    "            elif activation.lower() == 'sigmoid':\n",
    "                self.activation = F.sigmoid\n",
    "            elif activation.lower() == 'tanh':\n",
    "                self.activation = F.tanh\n",
    "        elif activation is None:\n",
    "            self.activation = lambda x: x\n",
    "        elif callable(activation):\n",
    "            self.activation = activation\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Undefined activation function: choose from: [relu, tanh, sigmoid, or a callable function]\")\n",
    "\n",
    "        logger.info(\"Start constructing character vocabulary.\")\n",
    "        # 建立char的词表\n",
    "        self.char_vocab = _construct_char_vocab_from_vocab(vocab, min_freq=min_char_freq,\n",
    "                                                           include_word_start_end=include_word_start_end)\n",
    "        self.char_pad_index = self.char_vocab.padding_idx\n",
    "        logger.info(f\"In total, there are {len(self.char_vocab)} distinct characters.\")\n",
    "        # 对vocab进行index\n",
    "        max_word_len = max(map(lambda x: len(x[0]), vocab))\n",
    "        if include_word_start_end:\n",
    "            max_word_len += 2\n",
    "        self.register_buffer('words_to_chars_embedding', torch.full((len(vocab), max_word_len),\n",
    "                                                                    fill_value=self.char_pad_index, dtype=torch.long))\n",
    "        self.register_buffer('word_lengths', torch.zeros(len(vocab)).long())\n",
    "        for word, index in vocab:\n",
    "            # if index!=vocab.padding_idx:  # 如果是pad的话，直接就为pad_value了. 修改为不区分pad与否\n",
    "            if include_word_start_end:\n",
    "                word = ['<bow>'] + list(word) + ['<eow>']\n",
    "            self.words_to_chars_embedding[index, :len(word)] = \\\n",
    "                torch.LongTensor([self.char_vocab.to_index(c) for c in word])\n",
    "            self.word_lengths[index] = len(word)\n",
    "\n",
    "        self.char_embedding = get_embeddings((len(self.char_vocab), char_emb_size))\n",
    "        self.transformer = TransformerEncoder(1, char_emb_size, char_n_head, char_dim_ffn, dropout=char_dropout, after_norm=char_after_norm,\n",
    "                                              attn_type=char_attn_type, pos_embed=char_pos_embed, scale=char_scale)\n",
    "        self.fc = nn.Linear(char_emb_size, embed_size)\n",
    "\n",
    "        self._embed_size = embed_size\n",
    "\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, words):\n",
    "        \"\"\"\n",
    "        输入words的index后，生成对应的words的表示。\n",
    "\n",
    "        :param words: [batch_size, max_len]\n",
    "        :return: [batch_size, max_len, embed_size]\n",
    "        \"\"\"\n",
    "        words = self.drop_word(words)\n",
    "        batch_size, max_len = words.size()\n",
    "        chars = self.words_to_chars_embedding[words]  # batch_size x max_len x max_word_len\n",
    "        word_lengths = self.word_lengths[words]  # batch_size x max_len\n",
    "        max_word_len = word_lengths.max()\n",
    "        chars = chars[:, :, :max_word_len]\n",
    "        # 为mask的地方为1\n",
    "        chars_masks = chars.eq(self.char_pad_index)  # batch_size x max_len x max_word_len 如果为0, 说明是padding的位置了\n",
    "        char_embeds = self.char_embedding(chars)  # batch_size x max_len x max_word_len x embed_size\n",
    "        char_embeds = self.dropout(char_embeds)\n",
    "        reshaped_chars = char_embeds.reshape(batch_size * max_len, max_word_len, -1)\n",
    "\n",
    "        trans_chars = self.transformer(reshaped_chars, chars_masks.eq(0).reshape(-1, max_word_len))\n",
    "        trans_chars = trans_chars.reshape(batch_size, max_len, max_word_len, -1)\n",
    "        trans_chars = self.activation(trans_chars)\n",
    "        if self.pool_method == 'max':\n",
    "            trans_chars = trans_chars.masked_fill(chars_masks.unsqueeze(-1), float('-inf'))\n",
    "            chars, _ = torch.max(trans_chars, dim=-2)  # batch_size x max_len x H\n",
    "        else:\n",
    "            trans_chars = trans_chars.masked_fill(chars_masks.unsqueeze(-1), 0)\n",
    "            chars = torch.sum(trans_chars, dim=-2) / chars_masks.eq(0).sum(dim=-1, keepdim=True).float()\n",
    "\n",
    "        chars = self.fc(chars)\n",
    "\n",
    "        return self.dropout(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeEmbedding(nn.Module):\n",
    "    def forward(self, input):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\n",
    "        \"\"\"\n",
    "        bsz, seq_len = input.size()\n",
    "        max_pos = self.padding_idx + seq_len\n",
    "        if max_pos > self.origin_shift:\n",
    "            # recompute/expand embeddings if needed\n",
    "            weights = self.get_embedding(\n",
    "                max_pos*2,\n",
    "                self.embedding_dim,\n",
    "                self.padding_idx,\n",
    "            )\n",
    "            weights = weights.to(self._float_tensor)\n",
    "            del self.weights\n",
    "            self.origin_shift = weights.size(0)//2\n",
    "            self.register_buffer('weights', weights)\n",
    "\n",
    "        positions = torch.arange(-seq_len, seq_len).to(input.device).long() + self.origin_shift  # 2*seq_len\n",
    "        embed = self.weights.index_select(0, positions.long()).detach()\n",
    "        return embed\n",
    "\n",
    "\n",
    "class RelativeSinusoidalPositionalEmbedding(RelativeEmbedding):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
    "    Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, padding_idx, init_size=1568):\n",
    "        \"\"\"\n",
    "\n",
    "        :param embedding_dim: 每个位置的dimension\n",
    "        :param padding_idx:\n",
    "        :param init_size:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        assert init_size%2==0\n",
    "        weights = self.get_embedding(\n",
    "            init_size+1,\n",
    "            embedding_dim,\n",
    "            padding_idx,\n",
    "        )\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.register_buffer('_float_tensor', torch.FloatTensor(1))\n",
    "\n",
    "    def get_embedding(self, num_embeddings, embedding_dim, padding_idx=None):\n",
    "        \"\"\"Build sinusoidal embeddings.\n",
    "        This matches the implementation in tensor2tensor, but differs slightly\n",
    "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "        \"\"\"\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
    "        emb = torch.arange(-num_embeddings//2, num_embeddings//2, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
    "        if embedding_dim % 2 == 1:\n",
    "            # zero pad\n",
    "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
    "        if padding_idx is not None:\n",
    "            emb[padding_idx, :] = 0\n",
    "        self.origin_shift = num_embeddings//2 + 1\n",
    "        return emb\n",
    "\n",
    "\n",
    "class RelativeMultiHeadAttn(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout, r_w_bias=None, r_r_bias=None, scale=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param int d_model:\n",
    "        :param int n_head:\n",
    "        :param dropout: 对attention map的dropout\n",
    "        :param r_w_bias: n_head x head_dim or None, 如果为dim\n",
    "        :param r_r_bias: n_head x head_dim or None,\n",
    "        :param scale:\n",
    "        :param rel_pos_embed:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.qv_linear = nn.Linear(d_model, d_model * 2, bias=False)\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        self.pos_embed = RelativeSinusoidalPositionalEmbedding(d_model//n_head, 0, 1200)\n",
    "\n",
    "        if scale:\n",
    "            self.scale = math.sqrt(d_model // n_head)\n",
    "        else:\n",
    "            self.scale = 1\n",
    "\n",
    "        if r_r_bias is None or r_w_bias is None:  # Biases are not shared\n",
    "            self.r_r_bias = nn.Parameter(nn.init.xavier_normal_(torch.zeros(n_head, d_model // n_head)))\n",
    "            self.r_w_bias = nn.Parameter(nn.init.xavier_normal_(torch.zeros(n_head, d_model // n_head)))\n",
    "        else:\n",
    "            self.r_r_bias = r_r_bias  # r_r_bias就是v\n",
    "            self.r_w_bias = r_w_bias  # r_w_bias就是u\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: batch_size x max_len x d_model\n",
    "        :param mask: batch_size x max_len\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, max_len, d_model = x.size()\n",
    "        pos_embed = self.pos_embed(mask)  # l x head_dim\n",
    "\n",
    "        qv = self.qv_linear(x)  # batch_size x max_len x d_model2\n",
    "        q, v = torch.chunk(qv, chunks=2, dim=-1)\n",
    "        q = q.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)\n",
    "        k = x.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)\n",
    "        v = v.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)  # b x n x l x d\n",
    "\n",
    "        rw_head_q = q + self.r_r_bias[:, None]\n",
    "        AC = torch.einsum('bnqd,bnkd->bnqk', [rw_head_q, k])  # b x n x l x d, n是head\n",
    "\n",
    "        D_ = torch.einsum('nd,ld->nl', self.r_w_bias, pos_embed)[None, :, None]  # head x 2max_len, 每个head对位置的bias\n",
    "        B_ = torch.einsum('bnqd,ld->bnql', q, pos_embed)  # bsz x head  x max_len x 2max_len，每个query对每个shift的偏移\n",
    "        BD = B_ + D_  # bsz x head x max_len x 2max_len, 要转换为bsz x head x max_len x max_len\n",
    "        BD = self._shift(BD)\n",
    "        attn = AC + BD\n",
    "\n",
    "        attn = attn / self.scale\n",
    "\n",
    "        attn = attn.masked_fill(mask[:, None, None, :].eq(0), float('-inf'))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout_layer(attn)\n",
    "        v = torch.matmul(attn, v).transpose(1, 2).reshape(batch_size, max_len, d_model)  # b x n x l x d\n",
    "\n",
    "        return v\n",
    "\n",
    "    def _shift(self, BD):\n",
    "        \"\"\"\n",
    "        类似\n",
    "        -3 -2 -1 0 1 2\n",
    "        -3 -2 -1 0 1 2\n",
    "        -3 -2 -1 0 1 2\n",
    "\n",
    "        转换为\n",
    "        0   1  2\n",
    "        -1  0  1\n",
    "        -2 -1  0\n",
    "\n",
    "        :param BD: batch_size x n_head x max_len x 2max_len\n",
    "        :return: batch_size x n_head x max_len x max_len\n",
    "        \"\"\"\n",
    "        bsz, n_head, max_len, _ = BD.size()\n",
    "        zero_pad = BD.new_zeros(bsz, n_head, max_len, 1)\n",
    "        BD = torch.cat([BD, zero_pad], dim=-1).view(bsz, n_head, -1, max_len)  # bsz x n_head x (2max_len+1) x max_len\n",
    "        BD = BD[:, :, :-1].view(bsz, n_head, max_len, -1)  # bsz x n_head x 2max_len x max_len\n",
    "        BD = BD[:, :, :, max_len:]\n",
    "        return BD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.1, scale=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param d_model:\n",
    "        :param n_head:\n",
    "        :param scale: 是否scale输出\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model%n_head==0\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.qkv_linear = nn.Linear(d_model, 3*d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        if scale:\n",
    "            self.scale = math.sqrt(d_model//n_head)\n",
    "        else:\n",
    "            self.scale = 1\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: bsz x max_len x d_model\n",
    "        :param mask: bsz x max_len\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size, max_len, d_model = x.size()\n",
    "        x = self.qkv_linear(x)\n",
    "        q, k, v = torch.chunk(x, 3, dim=-1)\n",
    "        q = q.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)\n",
    "        k = k.view(batch_size, max_len, self.n_head, -1).permute(0, 2, 3, 1)\n",
    "        v = v.view(batch_size, max_len, self.n_head, -1).transpose(1, 2)\n",
    "\n",
    "        attn = torch.matmul(q, k)  # batch_size x n_head x max_len x max_len\n",
    "        attn = attn/self.scale\n",
    "        attn.masked_fill_(mask=mask[:, None, None].eq(0), value=float('-inf'))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)  # batch_size x n_head x max_len x max_len\n",
    "        attn = self.dropout_layer(attn)\n",
    "        v = torch.matmul(attn, v)  # batch_size x n_head x max_len x d_model//n_head\n",
    "        v = v.transpose(1, 2).reshape(batch_size, max_len, -1)\n",
    "        v = self.fc(v)\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, self_attn, feedforward_dim, after_norm, dropout):\n",
    "        \"\"\"\n",
    "\n",
    "        :param int d_model: 一般512之类的\n",
    "        :param self_attn: self attention模块，输入为x:batch_size x max_len x d_model, mask:batch_size x max_len, 输出为\n",
    "            batch_size x max_len x d_model\n",
    "        :param int feedforward_dim: FFN中间层的dimension的大小\n",
    "        :param bool after_norm: norm的位置不一样，如果为False，则embedding可以直接连到输出\n",
    "        :param float dropout: 一共三个位置的dropout的大小\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.self_attn = self_attn\n",
    "\n",
    "        self.after_norm = after_norm\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, feedforward_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(dropout),\n",
    "                                 nn.Linear(feedforward_dim, d_model),\n",
    "                                 nn.Dropout(dropout))\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: batch_size x max_len x hidden_size\n",
    "        :param mask: batch_size x max_len, 为0的地方为pad\n",
    "        :return: batch_size x max_len x hidden_size\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        if not self.after_norm:\n",
    "            x = self.norm1(x)\n",
    "\n",
    "        x = self.self_attn(x, mask)\n",
    "        x = x + residual\n",
    "        if self.after_norm:\n",
    "            x = self.norm1(x)\n",
    "        residual = x\n",
    "        if not self.after_norm:\n",
    "            x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x\n",
    "        if self.after_norm:\n",
    "            x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, n_head, feedforward_dim, dropout, after_norm=True, attn_type='naive',\n",
    "                 scale=False, dropout_attn=None, pos_embed=None):\n",
    "        super().__init__()\n",
    "        if dropout_attn is None:\n",
    "            dropout_attn = dropout\n",
    "        self.d_model = d_model\n",
    "\n",
    "        if pos_embed is None:\n",
    "            self.pos_embed = None\n",
    "        elif pos_embed == 'sin':\n",
    "            self.pos_embed = SinusoidalPositionalEmbedding(d_model, 0, init_size=1024)\n",
    "        elif pos_embed == 'fix':\n",
    "            self.pos_embed = LearnedPositionalEmbedding(1024, d_model, 0)\n",
    "\n",
    "        if attn_type == 'transformer':\n",
    "            self_attn = MultiHeadAttn(d_model, n_head, dropout_attn, scale=scale)\n",
    "        elif attn_type == 'adatrans':\n",
    "            self_attn = RelativeMultiHeadAttn(d_model, n_head, dropout_attn, scale=scale)\n",
    "\n",
    "        self.layers = nn.ModuleList([TransformerLayer(d_model, deepcopy(self_attn), feedforward_dim, after_norm, dropout)\n",
    "                       for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: batch_size x max_len\n",
    "        :param mask: batch_size x max_len. 有value的地方为1\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed(mask)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_positions(tensor, padding_idx):\n",
    "    \"\"\"Replace non-padding symbols with their position numbers.\n",
    "    Position numbers begin at padding_idx+1. Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "    # The series of casts and type-conversions here are carefully\n",
    "    # balanced to both work with ONNX export and XLA. In particular XLA\n",
    "    # prefers ints, cumsum defaults to output longs, and ONNX doesn't know\n",
    "    # how to handle the dtype kwarg in cumsum.\n",
    "    mask = tensor.ne(padding_idx).int()\n",
    "    return (\n",
    "        torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
    "    ).long() + padding_idx\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
    "    Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, padding_idx, init_size=1568):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "            init_size,\n",
    "            embedding_dim,\n",
    "            padding_idx,\n",
    "        )\n",
    "        self.register_buffer('_float_tensor', torch.FloatTensor(1))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
    "        \"\"\"Build sinusoidal embeddings.\n",
    "        This matches the implementation in tensor2tensor, but differs slightly\n",
    "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "        \"\"\"\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
    "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
    "        if embedding_dim % 2 == 1:\n",
    "            # zero pad\n",
    "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
    "        if padding_idx is not None:\n",
    "            emb[padding_idx, :] = 0\n",
    "        return emb\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = input.size()\n",
    "        max_pos = self.padding_idx + 1 + seq_len\n",
    "        if max_pos > self.weights.size(0):\n",
    "            # recompute/expand embeddings if needed\n",
    "            self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "                max_pos,\n",
    "                self.embedding_dim,\n",
    "                self.padding_idx,\n",
    "            )\n",
    "        self.weights = self.weights.to(self._float_tensor)\n",
    "\n",
    "        positions = make_positions(input, self.padding_idx)\n",
    "        return self.weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum number of supported positions.\"\"\"\n",
    "        return int(1e5)  # an arbitrary large number\n",
    "\n",
    "\n",
    "class LearnedPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size.\n",
    "    Padding ids are ignored by either offsetting based on padding_idx\n",
    "    or by setting padding_idx to None and ensuring that the appropriate\n",
    "    position ids are passed to the forward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            padding_idx: int,\n",
    "    ):\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # positions: batch_size x max_len, 把words的index输入就好了\n",
    "        positions = make_positions(input, self.padding_idx)\n",
    "        return super().forward(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 4 : Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastNLP.modules import ConditionalRandomField, allowed_transitions\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TENER(nn.Module):\n",
    "    def __init__(self, tag_vocab, embed, num_layers, d_model, n_head, feedforward_dim, dropout,\n",
    "                 after_norm=True, attn_type='adatrans',  bi_embed=None,\n",
    "                 fc_dropout=0.3, pos_embed=None, scale=False, dropout_attn=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tag_vocab: fastNLP Vocabulary\n",
    "        :param embed: fastNLP TokenEmbedding\n",
    "        :param num_layers: number of self-attention layers\n",
    "        :param d_model: input size\n",
    "        :param n_head: number of head\n",
    "        :param feedforward_dim: the dimension of ffn\n",
    "        :param dropout: dropout in self-attention\n",
    "        :param after_norm: normalization place\n",
    "        :param attn_type: adatrans, naive\n",
    "        :param rel_pos_embed: position embedding的类型，支持sin, fix, None. relative时可为None\n",
    "        :param bi_embed: Used in Chinese scenerio\n",
    "        :param fc_dropout: dropout rate before the fc layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = embed\n",
    "        embed_size = self.embed.embed_size\n",
    "        self.bi_embed = None\n",
    "        if bi_embed is not None:\n",
    "            self.bi_embed = bi_embed\n",
    "            embed_size += self.bi_embed.embed_size\n",
    "\n",
    "        self.in_fc = nn.Linear(embed_size, d_model)\n",
    "\n",
    "        self.transformer = TransformerEncoder(num_layers, d_model, n_head, feedforward_dim, dropout,\n",
    "                                              after_norm=after_norm, attn_type=attn_type,\n",
    "                                              scale=scale, dropout_attn=dropout_attn,\n",
    "                                              pos_embed=pos_embed)\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.out_fc = nn.Linear(d_model, len(tag_vocab))\n",
    "\n",
    "        trans = allowed_transitions(tag_vocab, include_start_end=True)\n",
    "        self.crf = ConditionalRandomField(len(tag_vocab), include_start_end_trans=True, allowed_transitions=trans)\n",
    "\n",
    "    def _forward(self, chars, target, bigrams=None):\n",
    "        mask = chars.ne(0)\n",
    "        chars = self.embed(chars)\n",
    "        if self.bi_embed is not None:\n",
    "            bigrams = self.bi_embed(bigrams)\n",
    "            chars = torch.cat([chars, bigrams], dim=-1)\n",
    "\n",
    "        chars = self.in_fc(chars)\n",
    "        chars = self.transformer(chars, mask)\n",
    "        chars = self.fc_dropout(chars)\n",
    "        chars = self.out_fc(chars)\n",
    "        logits = F.log_softmax(chars, dim=-1)\n",
    "        if target is None:\n",
    "            paths, _ = self.crf.viterbi_decode(logits, mask)\n",
    "            return {'pred': paths}\n",
    "        else:\n",
    "            loss = self.crf(logits, target, mask)\n",
    "            return {'loss': loss}\n",
    "\n",
    "    def forward(self, chars, target, bigrams=None):\n",
    "        return self._forward(chars, target, bigrams)\n",
    "\n",
    "    def predict(self, chars, bigrams=None):\n",
    "        return self._forward(chars, target=None, bigrams=bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastNLP import Callback, Tester, DataSet\n",
    "\n",
    "\n",
    "class EvaluateCallback(Callback):\n",
    "    \"\"\"\n",
    "    通过使用该Callback可以使得Trainer在evaluate dev之外还可以evaluate其它数据集，比如测试集。每一次验证dev之前都会先验证EvaluateCallback\n",
    "    中的数据。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data=None, tester=None):\n",
    "        \"\"\"\n",
    "        :param ~fastNLP.DataSet,Dict[~fastNLP.DataSet] data: 传入DataSet对象，会使用Trainer中的metric对数据进行验证。如果需要传入多个\n",
    "            DataSet请通过dict的方式传入。\n",
    "        :param ~fastNLP.Tester,Dict[~fastNLP.DataSet] tester: Tester对象, 通过使用Tester对象，可以使得验证的metric与Trainer中\n",
    "            的metric不一样。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.datasets = {}\n",
    "        self.testers = {}\n",
    "        self.best_test_metric_sofar = 0\n",
    "        self.best_test_sofar = None\n",
    "        self.best_test_epoch = 0\n",
    "        self.best_dev_test = None\n",
    "        self.best_dev_epoch = 0\n",
    "        if tester is not None:\n",
    "            if isinstance(tester, dict):\n",
    "                for name, test in tester.items():\n",
    "                    if not isinstance(test, Tester):\n",
    "                        raise TypeError(f\"{name} in tester is not a valid fastNLP.Tester.\")\n",
    "                    self.testers['tester-' + name] = test\n",
    "            if isinstance(tester, Tester):\n",
    "                self.testers['tester-test'] = tester\n",
    "            for tester in self.testers.values():\n",
    "                setattr(tester, 'verbose', 0)\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                assert isinstance(value, DataSet), f\"Only DataSet object is allowed, not {type(value)}.\"\n",
    "            for key, value in data.items():\n",
    "                self.datasets['data-' + key] = value\n",
    "        elif isinstance(data, DataSet):\n",
    "            self.datasets['data-test'] = data\n",
    "        elif data is not None:\n",
    "            raise TypeError(\"data receives dict[DataSet] or DataSet object.\")\n",
    "\n",
    "    def on_train_begin(self):\n",
    "        if len(self.datasets) > 0 and self.trainer.dev_data is None:\n",
    "            raise RuntimeError(\"Trainer has no dev data, you cannot pass extra DataSet to do evaluation.\")\n",
    "\n",
    "        if len(self.datasets) > 0:\n",
    "            for key, data in self.datasets.items():\n",
    "                tester = Tester(data=data, model=self.model,\n",
    "                                batch_size=self.trainer.kwargs.get('dev_batch_size', self.batch_size),\n",
    "                                metrics=self.trainer.metrics, verbose=0,\n",
    "                                use_tqdm=self.trainer.test_use_tqdm)\n",
    "                self.testers[key] = tester\n",
    "\n",
    "    def on_valid_end(self, eval_result, metric_key, optimizer, better_result):\n",
    "        if len(self.testers) > 0:\n",
    "            for idx, (key, tester) in enumerate(self.testers.items()):\n",
    "                try:\n",
    "                    eval_result = tester.test()\n",
    "                    if idx == 0:\n",
    "                        indicator, indicator_val = _check_eval_results(eval_result)\n",
    "                        if indicator_val>self.best_test_metric_sofar:\n",
    "                            self.best_test_metric_sofar = indicator_val\n",
    "                            self.best_test_epoch = self.epoch\n",
    "                            self.best_test_sofar = eval_result\n",
    "                    if better_result:\n",
    "                        self.best_dev_test = eval_result\n",
    "                        self.best_dev_epoch = self.epoch\n",
    "                    self.logger.info(\"EvaluateCallback evaluation on {}:\".format(key))\n",
    "                    self.logger.info(tester._format_eval_results(eval_result))\n",
    "                except Exception as e:\n",
    "                    self.logger.error(\"Exception happens when evaluate on DataSet named `{}`.\".format(key))\n",
    "                    raise e\n",
    "\n",
    "    def on_train_end(self):\n",
    "        if self.best_test_sofar:\n",
    "            self.logger.info(\"Best test performance(may not correspond to the best dev performance):{} achieved at Epoch:{}.\".format(self.best_test_sofar, self.best_test_epoch))\n",
    "        if self.best_dev_test:\n",
    "            self.logger.info(\"Best test performance(correspond to the best dev performance):{} achieved at Epoch:{}.\".format(self.best_dev_test, self.best_dev_epoch))\n",
    "\n",
    "\n",
    "def _check_eval_results(metrics, metric_key=None):\n",
    "    # metrics: tester返回的结果\n",
    "    # metric_key: 一个用来做筛选的指标，来自Trainer的初始化\n",
    "    if isinstance(metrics, tuple):\n",
    "        loss, metrics = metrics\n",
    "\n",
    "    if isinstance(metrics, dict):\n",
    "        metric_dict = list(metrics.values())[0]  # 取第一个metric\n",
    "\n",
    "        if metric_key is None:\n",
    "            indicator_val, indicator = list(metric_dict.values())[0], list(metric_dict.keys())[0]\n",
    "        else:\n",
    "            # metric_key is set\n",
    "            if metric_key not in metric_dict:\n",
    "                raise RuntimeError(f\"metric key {metric_key} not found in {metric_dict}\")\n",
    "            indicator_val = metric_dict[metric_key]\n",
    "            indicator = metric_key\n",
    "    else:\n",
    "        raise RuntimeError(\"Invalid metrics type. Expect {}, got {}\".format((tuple, dict), type(metrics)))\n",
    "    return indicator, indicator_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read cache from caches/en-ontonotes_transformer_bioes_adatrans_True.pkl.\n",
      "In total 3 datasets:\n",
      "\ttest has 3453 instances.\n",
      "\ttrain has 14041 instances.\n",
      "\tdev has 3250 instances.\n",
      "In total 2 vocabs:\n",
      "\ttarget has 17 entries.\n",
      "\tchars has 25305 entries.\n",
      "\n",
      "input fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 9]) \n",
      "\tseq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\tchars: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 9]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 9]) \n",
      "\tseq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2020-02-19-13-11-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3456b45c53247118d7a91aa0b797840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=87800.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 8.06 seconds!\n",
      "Evaluate data in 6.53 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.659051, pre=0.631221, rec=0.689448\n",
      "Evaluation on dev at Epoch 1/100. Step:878/87800: \n",
      "SpanFPreRecMetric: f=0.684282, pre=0.654478, rec=0.71693\n",
      "\n",
      "Evaluate data in 6.65 seconds!\n",
      "Evaluate data in 6.94 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.804377, pre=0.801759, rec=0.807011\n",
      "Evaluation on dev at Epoch 2/100. Step:1756/87800: \n",
      "SpanFPreRecMetric: f=0.843104, pre=0.841549, rec=0.844665\n",
      "\n",
      "Evaluate data in 6.62 seconds!\n",
      "Evaluate data in 6.52 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.841516, pre=0.831978, rec=0.851275\n",
      "Evaluation on dev at Epoch 3/100. Step:2634/87800: \n",
      "SpanFPreRecMetric: f=0.873985, pre=0.869688, rec=0.878324\n",
      "\n",
      "Evaluate data in 9.96 seconds!\n",
      "Evaluate data in 9.77 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.854029, pre=0.861882, rec=0.846317\n",
      "Evaluation on dev at Epoch 4/100. Step:3512/87800: \n",
      "SpanFPreRecMetric: f=0.890553, pre=0.900826, rec=0.880512\n",
      "\n",
      "Evaluate data in 8.96 seconds!\n",
      "Evaluate data in 9.64 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.865753, pre=0.864302, rec=0.86721\n",
      "Evaluation on dev at Epoch 5/100. Step:4390/87800: \n",
      "SpanFPreRecMetric: f=0.902818, pre=0.905108, rec=0.900539\n",
      "\n",
      "Evaluate data in 8.4 seconds!\n",
      "Evaluate data in 9.52 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.869865, pre=0.869865, rec=0.869865\n",
      "Evaluation on dev at Epoch 6/100. Step:5268/87800: \n",
      "SpanFPreRecMetric: f=0.908968, pre=0.911352, rec=0.906597\n",
      "\n",
      "Evaluate data in 9.61 seconds!\n",
      "Evaluate data in 9.75 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.878087, pre=0.87188, rec=0.884384\n",
      "Evaluation on dev at Epoch 7/100. Step:6146/87800: \n",
      "SpanFPreRecMetric: f=0.914815, pre=0.915123, rec=0.914507\n",
      "\n",
      "Evaluate data in 9.6 seconds!\n",
      "Evaluate data in 9.5 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.878637, pre=0.880512, rec=0.876771\n",
      "Evaluation on dev at Epoch 8/100. Step:7024/87800: \n",
      "SpanFPreRecMetric: f=0.918549, pre=0.923312, rec=0.913834\n",
      "\n",
      "Evaluate data in 6.61 seconds!\n",
      "Evaluate data in 6.7 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.883552, pre=0.884493, rec=0.882613\n",
      "Evaluation on dev at Epoch 9/100. Step:7902/87800: \n",
      "SpanFPreRecMetric: f=0.91912, pre=0.924124, rec=0.91417\n",
      "\n",
      "Evaluate data in 6.66 seconds!\n",
      "Evaluate data in 6.97 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.890268, pre=0.891374, rec=0.889164\n",
      "Evaluation on dev at Epoch 10/100. Step:8780/87800: \n",
      "SpanFPreRecMetric: f=0.925249, pre=0.92981, rec=0.920734\n",
      "\n",
      "Evaluate data in 6.4 seconds!\n",
      "Evaluate data in 6.56 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.890395, pre=0.885484, rec=0.895361\n",
      "Evaluation on dev at Epoch 11/100. Step:9658/87800: \n",
      "SpanFPreRecMetric: f=0.925783, pre=0.925783, rec=0.925783\n",
      "\n",
      "Evaluate data in 6.5 seconds!\n",
      "Evaluate data in 6.52 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.891487, pre=0.89204, rec=0.890935\n",
      "Evaluation on dev at Epoch 12/100. Step:10536/87800: \n",
      "SpanFPreRecMetric: f=0.925055, pre=0.929932, rec=0.920229\n",
      "\n",
      "Evaluate data in 6.91 seconds!\n",
      "Evaluate data in 6.69 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.890483, pre=0.890561, rec=0.890404\n",
      "Evaluation on dev at Epoch 13/100. Step:11414/87800: \n",
      "SpanFPreRecMetric: f=0.927548, pre=0.928644, rec=0.926456\n",
      "\n",
      "Evaluate data in 6.37 seconds!\n",
      "Evaluate data in 7.06 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.889992, pre=0.891177, rec=0.88881\n",
      "Evaluation on dev at Epoch 14/100. Step:12292/87800: \n",
      "SpanFPreRecMetric: f=0.927938, pre=0.931637, rec=0.924268\n",
      "\n",
      "Evaluate data in 6.81 seconds!\n",
      "Evaluate data in 6.8 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.893161, pre=0.885774, rec=0.900673\n",
      "Evaluation on dev at Epoch 15/100. Step:13170/87800: \n",
      "SpanFPreRecMetric: f=0.927743, pre=0.926342, rec=0.929148\n",
      "\n",
      "Evaluate data in 6.8 seconds!\n",
      "Evaluate data in 6.87 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.892854, pre=0.890887, rec=0.89483\n",
      "Evaluation on dev at Epoch 16/100. Step:14048/87800: \n",
      "SpanFPreRecMetric: f=0.928523, pre=0.930091, rec=0.926961\n",
      "\n",
      "Evaluate data in 6.39 seconds!\n",
      "Evaluate data in 6.53 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.900564, pre=0.89677, rec=0.904391\n",
      "Evaluation on dev at Epoch 17/100. Step:14926/87800: \n",
      "SpanFPreRecMetric: f=0.929364, pre=0.928739, rec=0.92999\n",
      "\n",
      "Evaluate data in 6.78 seconds!\n",
      "Evaluate data in 6.91 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.898538, pre=0.899415, rec=0.897663\n",
      "Evaluation on dev at Epoch 18/100. Step:15804/87800: \n",
      "SpanFPreRecMetric: f=0.931622, pre=0.934621, rec=0.928644\n",
      "\n",
      "Evaluate data in 6.42 seconds!\n",
      "Evaluate data in 6.53 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.899761, pre=0.900799, rec=0.898725\n",
      "Evaluation on dev at Epoch 19/100. Step:16682/87800: \n",
      "SpanFPreRecMetric: f=0.933232, pre=0.936156, rec=0.930326\n",
      "\n",
      "Evaluate data in 6.56 seconds!\n",
      "Evaluate data in 6.76 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.899471, pre=0.895994, rec=0.902975\n",
      "Evaluation on dev at Epoch 20/100. Step:17560/87800: \n",
      "SpanFPreRecMetric: f=0.935908, pre=0.937965, rec=0.933861\n",
      "\n",
      "Evaluate data in 6.58 seconds!\n",
      "Evaluate data in 6.59 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.89703, pre=0.901036, rec=0.893059\n",
      "Evaluation on dev at Epoch 21/100. Step:18438/87800: \n",
      "SpanFPreRecMetric: f=0.936548, pre=0.941647, rec=0.931505\n",
      "\n",
      "Evaluate data in 6.46 seconds!\n",
      "Evaluate data in 6.63 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.892509, pre=0.892667, rec=0.892351\n",
      "Evaluation on dev at Epoch 22/100. Step:19316/87800: \n",
      "SpanFPreRecMetric: f=0.93411, pre=0.937754, rec=0.930495\n",
      "\n",
      "Evaluate data in 6.51 seconds!\n",
      "Evaluate data in 6.76 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.902244, pre=0.907414, rec=0.897132\n",
      "Evaluation on dev at Epoch 23/100. Step:20194/87800: \n",
      "SpanFPreRecMetric: f=0.937421, pre=0.943412, rec=0.931505\n",
      "\n",
      "Evaluate data in 4.96 seconds!\n",
      "Evaluate data in 4.91 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.901531, pre=0.900973, rec=0.902089\n",
      "Evaluation on dev at Epoch 24/100. Step:21072/87800: \n",
      "SpanFPreRecMetric: f=0.937215, pre=0.939912, rec=0.934534\n",
      "\n",
      "Evaluate data in 4.84 seconds!\n",
      "Evaluate data in 5.03 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.904847, pre=0.910689, rec=0.899079\n",
      "Evaluation on dev at Epoch 25/100. Step:21950/87800: \n",
      "SpanFPreRecMetric: f=0.941376, pre=0.947799, rec=0.935039\n",
      "\n",
      "Evaluate data in 4.83 seconds!\n",
      "Evaluate data in 4.93 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.897535, pre=0.892507, rec=0.90262\n",
      "Evaluation on dev at Epoch 26/100. Step:22828/87800: \n",
      "SpanFPreRecMetric: f=0.93822, pre=0.937196, rec=0.939246\n",
      "\n",
      "Evaluate data in 4.84 seconds!\n",
      "Evaluate data in 4.95 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.903123, pre=0.902564, rec=0.903683\n",
      "Evaluation on dev at Epoch 27/100. Step:23706/87800: \n",
      "SpanFPreRecMetric: f=0.939192, pre=0.941336, rec=0.937058\n",
      "\n",
      "Evaluate data in 4.84 seconds!\n",
      "Evaluate data in 5.01 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.904369, pre=0.905413, rec=0.903329\n",
      "Evaluation on dev at Epoch 28/100. Step:24584/87800: \n",
      "SpanFPreRecMetric: f=0.938586, pre=0.940968, rec=0.936217\n",
      "\n",
      "Evaluate data in 4.82 seconds!\n",
      "Evaluate data in 4.94 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.903106, pre=0.902707, rec=0.903506\n",
      "Evaluation on dev at Epoch 29/100. Step:25462/87800: \n",
      "SpanFPreRecMetric: f=0.940084, pre=0.942789, rec=0.937395\n",
      "\n",
      "Evaluate data in 5.28 seconds!\n",
      "Evaluate data in 7.6 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.90428, pre=0.907018, rec=0.901558\n",
      "Evaluation on dev at Epoch 30/100. Step:26340/87800: \n",
      "SpanFPreRecMetric: f=0.941087, pre=0.945322, rec=0.93689\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 4.89 seconds!\n",
      "Evaluate data in 5.06 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.904736, pre=0.909921, rec=0.89961\n",
      "Evaluation on dev at Epoch 31/100. Step:27218/87800: \n",
      "SpanFPreRecMetric: f=0.941555, pre=0.946438, rec=0.936722\n",
      "\n",
      "Evaluate data in 4.76 seconds!\n",
      "Evaluate data in 4.9 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.899427, pre=0.895559, rec=0.903329\n",
      "Evaluation on dev at Epoch 32/100. Step:28096/87800: \n",
      "SpanFPreRecMetric: f=0.938889, pre=0.939205, rec=0.938573\n",
      "\n",
      "Evaluate data in 4.88 seconds!\n",
      "Evaluate data in 5.07 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.905533, pre=0.909903, rec=0.901204\n",
      "Evaluation on dev at Epoch 33/100. Step:28974/87800: \n",
      "SpanFPreRecMetric: f=0.939686, pre=0.943351, rec=0.936048\n",
      "\n",
      "Evaluate data in 4.98 seconds!\n",
      "Evaluate data in 5.3 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.902849, pre=0.90237, rec=0.903329\n",
      "Evaluation on dev at Epoch 34/100. Step:29852/87800: \n",
      "SpanFPreRecMetric: f=0.939877, pre=0.941862, rec=0.9379\n",
      "\n",
      "Evaluate data in 5.15 seconds!\n",
      "Evaluate data in 4.93 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.904249, pre=0.90984, rec=0.898725\n",
      "Evaluation on dev at Epoch 35/100. Step:30730/87800: \n",
      "SpanFPreRecMetric: f=0.94052, pre=0.945721, rec=0.935375\n",
      "\n",
      "Evaluate data in 4.83 seconds!\n",
      "Evaluate data in 4.98 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907198, pre=0.911823, rec=0.90262\n",
      "Evaluation on dev at Epoch 36/100. Step:31608/87800: \n",
      "SpanFPreRecMetric: f=0.942818, pre=0.947789, rec=0.9379\n",
      "\n",
      "Evaluate data in 4.82 seconds!\n",
      "Evaluate data in 4.87 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907123, pre=0.908978, rec=0.905276\n",
      "Evaluation on dev at Epoch 37/100. Step:32486/87800: \n",
      "SpanFPreRecMetric: f=0.942486, pre=0.944557, rec=0.940424\n",
      "\n",
      "Evaluate data in 4.77 seconds!\n",
      "Evaluate data in 4.93 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.903471, pre=0.903792, rec=0.903152\n",
      "Evaluation on dev at Epoch 38/100. Step:33364/87800: \n",
      "SpanFPreRecMetric: f=0.941117, pre=0.943505, rec=0.938741\n",
      "\n",
      "Evaluate data in 4.95 seconds!\n",
      "Evaluate data in 4.9 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907544, pre=0.908752, rec=0.906339\n",
      "Evaluation on dev at Epoch 39/100. Step:34242/87800: \n",
      "SpanFPreRecMetric: f=0.941861, pre=0.944491, rec=0.939246\n",
      "\n",
      "Evaluate data in 4.89 seconds!\n",
      "Evaluate data in 4.92 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909982, pre=0.916188, rec=0.90386\n",
      "Evaluation on dev at Epoch 40/100. Step:35120/87800: \n",
      "SpanFPreRecMetric: f=0.943017, pre=0.947503, rec=0.938573\n",
      "\n",
      "Evaluate data in 4.87 seconds!\n",
      "Evaluate data in 4.87 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.904051, pre=0.907275, rec=0.90085\n",
      "Evaluation on dev at Epoch 41/100. Step:35998/87800: \n",
      "SpanFPreRecMetric: f=0.938603, pre=0.942024, rec=0.935207\n",
      "\n",
      "Evaluate data in 4.85 seconds!\n",
      "Evaluate data in 4.97 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.904433, pre=0.905717, rec=0.903152\n",
      "Evaluation on dev at Epoch 42/100. Step:36876/87800: \n",
      "SpanFPreRecMetric: f=0.942368, pre=0.944999, rec=0.939751\n",
      "\n",
      "Evaluate data in 4.91 seconds!\n",
      "Evaluate data in 5.07 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.903249, pre=0.903169, rec=0.903329\n",
      "Evaluation on dev at Epoch 43/100. Step:37754/87800: \n",
      "SpanFPreRecMetric: f=0.943091, pre=0.944923, rec=0.941266\n",
      "\n",
      "Evaluate data in 4.95 seconds!\n",
      "Evaluate data in 4.97 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.903534, pre=0.901622, rec=0.905453\n",
      "Evaluation on dev at Epoch 44/100. Step:38632/87800: \n",
      "SpanFPreRecMetric: f=0.943517, pre=0.94527, rec=0.94177\n",
      "\n",
      "Evaluate data in 4.91 seconds!\n",
      "Evaluate data in 5.04 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.906844, pre=0.910568, rec=0.903152\n",
      "Evaluation on dev at Epoch 45/100. Step:39510/87800: \n",
      "SpanFPreRecMetric: f=0.940739, pre=0.945133, rec=0.936385\n",
      "\n",
      "Evaluate data in 4.92 seconds!\n",
      "Evaluate data in 5.03 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907253, pre=0.908703, rec=0.905807\n",
      "Evaluation on dev at Epoch 46/100. Step:40388/87800: \n",
      "SpanFPreRecMetric: f=0.942527, pre=0.945319, rec=0.939751\n",
      "\n",
      "Evaluate data in 5.04 seconds!\n",
      "Evaluate data in 4.89 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909091, pre=0.914383, rec=0.90386\n",
      "Evaluation on dev at Epoch 47/100. Step:41266/87800: \n",
      "SpanFPreRecMetric: f=0.943932, pre=0.948666, rec=0.939246\n",
      "\n",
      "Evaluate data in 4.84 seconds!\n",
      "Evaluate data in 4.98 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909835, pre=0.914802, rec=0.904922\n",
      "Evaluation on dev at Epoch 48/100. Step:42144/87800: \n",
      "SpanFPreRecMetric: f=0.943961, pre=0.948209, rec=0.939751\n",
      "\n",
      "Evaluate data in 4.98 seconds!\n",
      "Evaluate data in 5.08 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908185, pre=0.912732, rec=0.903683\n",
      "Evaluation on dev at Epoch 49/100. Step:43022/87800: \n",
      "SpanFPreRecMetric: f=0.943383, pre=0.947386, rec=0.939414\n",
      "\n",
      "Evaluate data in 5.0 seconds!\n",
      "Evaluate data in 4.93 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907672, pre=0.909366, rec=0.905984\n",
      "Evaluation on dev at Epoch 50/100. Step:43900/87800: \n",
      "SpanFPreRecMetric: f=0.944046, pre=0.946843, rec=0.941266\n",
      "\n",
      "Evaluate data in 5.08 seconds!\n",
      "Evaluate data in 5.02 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.906181, pre=0.910309, rec=0.902089\n",
      "Evaluation on dev at Epoch 51/100. Step:44778/87800: \n",
      "SpanFPreRecMetric: f=0.941107, pre=0.94502, rec=0.937227\n",
      "\n",
      "Evaluate data in 4.92 seconds!\n",
      "Evaluate data in 5.03 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907783, pre=0.91102, rec=0.904568\n",
      "Evaluation on dev at Epoch 52/100. Step:45656/87800: \n",
      "SpanFPreRecMetric: f=0.944745, pre=0.948592, rec=0.940929\n",
      "\n",
      "Evaluate data in 4.81 seconds!\n",
      "Evaluate data in 5.17 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908801, pre=0.91074, rec=0.90687\n",
      "Evaluation on dev at Epoch 53/100. Step:46534/87800: \n",
      "SpanFPreRecMetric: f=0.944534, pre=0.947654, rec=0.941434\n",
      "\n",
      "Evaluate data in 4.98 seconds!\n",
      "Evaluate data in 4.96 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909623, pre=0.91132, rec=0.907932\n",
      "Evaluation on dev at Epoch 54/100. Step:47412/87800: \n",
      "SpanFPreRecMetric: f=0.945814, pre=0.948696, rec=0.942949\n",
      "\n",
      "Evaluate data in 4.89 seconds!\n",
      "Evaluate data in 4.97 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.904355, pre=0.898898, rec=0.90988\n",
      "Evaluation on dev at Epoch 55/100. Step:48290/87800: \n",
      "SpanFPreRecMetric: f=0.943472, pre=0.943155, rec=0.94379\n",
      "\n",
      "Evaluate data in 4.88 seconds!\n",
      "Evaluate data in 4.86 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.910821, pre=0.914806, rec=0.90687\n",
      "Evaluation on dev at Epoch 56/100. Step:49168/87800: \n",
      "SpanFPreRecMetric: f=0.944219, pre=0.948387, rec=0.940088\n",
      "\n",
      "Evaluate data in 4.87 seconds!\n",
      "Evaluate data in 4.85 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909736, pre=0.912084, rec=0.907401\n",
      "Evaluation on dev at Epoch 57/100. Step:50046/87800: \n",
      "SpanFPreRecMetric: f=0.943689, pre=0.946807, rec=0.940592\n",
      "\n",
      "Evaluate data in 4.81 seconds!\n",
      "Evaluate data in 5.07 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909268, pre=0.909187, rec=0.909348\n",
      "Evaluation on dev at Epoch 58/100. Step:50924/87800: \n",
      "SpanFPreRecMetric: f=0.945682, pre=0.94792, rec=0.943453\n",
      "\n",
      "Evaluate data in 4.85 seconds!\n",
      "Evaluate data in 4.97 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908719, pre=0.913269, rec=0.904214\n",
      "Evaluation on dev at Epoch 59/100. Step:51802/87800: \n",
      "SpanFPreRecMetric: f=0.945464, pre=0.950042, rec=0.940929\n",
      "\n",
      "Evaluate data in 4.97 seconds!\n",
      "Evaluate data in 5.28 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.906886, pre=0.906726, rec=0.907047\n",
      "Evaluation on dev at Epoch 60/100. Step:52680/87800: \n",
      "SpanFPreRecMetric: f=0.943737, pre=0.946051, rec=0.941434\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 4.93 seconds!\n",
      "Evaluate data in 5.06 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908576, pre=0.907453, rec=0.909703\n",
      "Evaluation on dev at Epoch 61/100. Step:53558/87800: \n",
      "SpanFPreRecMetric: f=0.943209, pre=0.944482, rec=0.941939\n",
      "\n",
      "Evaluate data in 5.09 seconds!\n",
      "Evaluate data in 5.17 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908785, pre=0.909107, rec=0.908463\n",
      "Evaluation on dev at Epoch 62/100. Step:54436/87800: \n",
      "SpanFPreRecMetric: f=0.945025, pre=0.946942, rec=0.943117\n",
      "\n",
      "Evaluate data in 4.77 seconds!\n",
      "Evaluate data in 5.0 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907012, pre=0.908219, rec=0.905807\n",
      "Evaluation on dev at Epoch 63/100. Step:55314/87800: \n",
      "SpanFPreRecMetric: f=0.945166, pre=0.947564, rec=0.94278\n",
      "\n",
      "Evaluate data in 4.94 seconds!\n",
      "Evaluate data in 5.04 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.906112, pre=0.906593, rec=0.90563\n",
      "Evaluation on dev at Epoch 64/100. Step:56192/87800: \n",
      "SpanFPreRecMetric: f=0.94616, pre=0.948883, rec=0.943453\n",
      "\n",
      "Evaluate data in 4.88 seconds!\n",
      "Evaluate data in 4.91 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908688, pre=0.91087, rec=0.906516\n",
      "Evaluation on dev at Epoch 65/100. Step:57070/87800: \n",
      "SpanFPreRecMetric: f=0.945307, pre=0.948188, rec=0.942444\n",
      "\n",
      "Evaluate data in 4.77 seconds!\n",
      "Evaluate data in 5.06 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907107, pre=0.909123, rec=0.905099\n",
      "Evaluation on dev at Epoch 66/100. Step:57948/87800: \n",
      "SpanFPreRecMetric: f=0.945372, pre=0.947128, rec=0.943622\n",
      "\n",
      "Evaluate data in 4.95 seconds!\n",
      "Evaluate data in 4.98 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908397, pre=0.910822, rec=0.905984\n",
      "Evaluation on dev at Epoch 67/100. Step:58826/87800: \n",
      "SpanFPreRecMetric: f=0.945645, pre=0.948527, rec=0.94278\n",
      "\n",
      "Evaluate data in 4.93 seconds!\n",
      "Evaluate data in 4.87 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907398, pre=0.909172, rec=0.90563\n",
      "Evaluation on dev at Epoch 68/100. Step:59704/87800: \n",
      "SpanFPreRecMetric: f=0.943985, pre=0.94638, rec=0.941602\n",
      "\n",
      "Evaluate data in 4.98 seconds!\n",
      "Evaluate data in 4.95 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909139, pre=0.912137, rec=0.906161\n",
      "Evaluation on dev at Epoch 69/100. Step:60582/87800: \n",
      "SpanFPreRecMetric: f=0.945049, pre=0.94801, rec=0.942107\n",
      "\n",
      "Evaluate data in 5.04 seconds!\n",
      "Evaluate data in 4.95 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908881, pre=0.913596, rec=0.904214\n",
      "Evaluation on dev at Epoch 70/100. Step:61460/87800: \n",
      "SpanFPreRecMetric: f=0.944966, pre=0.94938, rec=0.940592\n",
      "\n",
      "Evaluate data in 4.84 seconds!\n",
      "Evaluate data in 4.89 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.911979, pre=0.914088, rec=0.90988\n",
      "Evaluation on dev at Epoch 71/100. Step:62338/87800: \n",
      "SpanFPreRecMetric: f=0.944632, pre=0.947511, rec=0.94177\n",
      "\n",
      "Evaluate data in 4.96 seconds!\n",
      "Evaluate data in 5.06 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909268, pre=0.910074, rec=0.908463\n",
      "Evaluation on dev at Epoch 72/100. Step:63216/87800: \n",
      "SpanFPreRecMetric: f=0.944918, pre=0.947235, rec=0.942612\n",
      "\n",
      "Evaluate data in 5.05 seconds!\n",
      "Evaluate data in 4.94 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.912113, pre=0.910986, rec=0.913244\n",
      "Evaluation on dev at Epoch 73/100. Step:64094/87800: \n",
      "SpanFPreRecMetric: f=0.945657, pre=0.946853, rec=0.944463\n",
      "\n",
      "Evaluate data in 5.06 seconds!\n",
      "Evaluate data in 5.22 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909461, pre=0.90922, rec=0.909703\n",
      "Evaluation on dev at Epoch 74/100. Step:64972/87800: \n",
      "SpanFPreRecMetric: f=0.945559, pre=0.946995, rec=0.944127\n",
      "\n",
      "Evaluate data in 5.01 seconds!\n",
      "Evaluate data in 4.89 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909397, pre=0.9098, rec=0.908994\n",
      "Evaluation on dev at Epoch 75/100. Step:65850/87800: \n",
      "SpanFPreRecMetric: f=0.945344, pre=0.947582, rec=0.943117\n",
      "\n",
      "Evaluate data in 4.96 seconds!\n",
      "Evaluate data in 5.12 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909816, pre=0.910461, rec=0.909171\n",
      "Evaluation on dev at Epoch 76/100. Step:66728/87800: \n",
      "SpanFPreRecMetric: f=0.944847, pre=0.946924, rec=0.94278\n",
      "\n",
      "Evaluate data in 4.94 seconds!\n",
      "Evaluate data in 5.02 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907319, pre=0.908123, rec=0.906516\n",
      "Evaluation on dev at Epoch 77/100. Step:67606/87800: \n",
      "SpanFPreRecMetric: f=0.94624, pre=0.949044, rec=0.943453\n",
      "\n",
      "Evaluate data in 4.9 seconds!\n",
      "Evaluate data in 4.89 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909107, pre=0.909752, rec=0.908463\n",
      "Evaluation on dev at Epoch 78/100. Step:68484/87800: \n",
      "SpanFPreRecMetric: f=0.944669, pre=0.946906, rec=0.942444\n",
      "\n",
      "Evaluate data in 4.89 seconds!\n",
      "Evaluate data in 5.0 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.90835, pre=0.909478, rec=0.907224\n",
      "Evaluation on dev at Epoch 79/100. Step:69362/87800: \n",
      "SpanFPreRecMetric: f=0.945992, pre=0.948714, rec=0.943285\n",
      "\n",
      "Evaluate data in 5.09 seconds!\n",
      "Evaluate data in 5.04 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908366, pre=0.909333, rec=0.907401\n",
      "Evaluation on dev at Epoch 80/100. Step:70240/87800: \n",
      "SpanFPreRecMetric: f=0.945921, pre=0.948401, rec=0.943453\n",
      "\n",
      "Evaluate data in 4.9 seconds!\n",
      "Evaluate data in 5.0 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908946, pre=0.909429, rec=0.908463\n",
      "Evaluation on dev at Epoch 81/100. Step:71118/87800: \n",
      "SpanFPreRecMetric: f=0.945939, pre=0.948098, rec=0.94379\n",
      "\n",
      "Evaluate data in 4.98 seconds!\n",
      "Evaluate data in 5.03 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.910507, pre=0.910427, rec=0.910588\n",
      "Evaluation on dev at Epoch 82/100. Step:71996/87800: \n",
      "SpanFPreRecMetric: f=0.945415, pre=0.947894, rec=0.942949\n",
      "\n",
      "Evaluate data in 4.78 seconds!\n",
      "Evaluate data in 4.92 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908656, pre=0.91027, rec=0.907047\n",
      "Evaluation on dev at Epoch 83/100. Step:72874/87800: \n",
      "SpanFPreRecMetric: f=0.944669, pre=0.946906, rec=0.942444\n",
      "\n",
      "Evaluate data in 4.97 seconds!\n",
      "Evaluate data in 5.05 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909574, pre=0.910866, rec=0.908286\n",
      "Evaluation on dev at Epoch 84/100. Step:73752/87800: \n",
      "SpanFPreRecMetric: f=0.946844, pre=0.949408, rec=0.944295\n",
      "\n",
      "Evaluate data in 4.95 seconds!\n",
      "Evaluate data in 5.07 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907136, pre=0.910616, rec=0.903683\n",
      "Evaluation on dev at Epoch 85/100. Step:74630/87800: \n",
      "SpanFPreRecMetric: f=0.944773, pre=0.948136, rec=0.941434\n",
      "\n",
      "Evaluate data in 4.83 seconds!\n",
      "Evaluate data in 5.16 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908317, pre=0.909769, rec=0.90687\n",
      "Evaluation on dev at Epoch 86/100. Step:75508/87800: \n",
      "SpanFPreRecMetric: f=0.945565, pre=0.948366, rec=0.94278\n",
      "\n",
      "Evaluate data in 5.04 seconds!\n",
      "Evaluate data in 5.18 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908043, pre=0.909575, rec=0.906516\n",
      "Evaluation on dev at Epoch 87/100. Step:76386/87800: \n",
      "SpanFPreRecMetric: f=0.943995, pre=0.946229, rec=0.94177\n",
      "\n",
      "Evaluate data in 4.91 seconds!\n",
      "Evaluate data in 4.95 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.909493, pre=0.908048, rec=0.910942\n",
      "Evaluation on dev at Epoch 88/100. Step:77264/87800: \n",
      "SpanFPreRecMetric: f=0.945844, pre=0.946721, rec=0.944968\n",
      "\n",
      "Evaluate data in 4.96 seconds!\n",
      "Evaluate data in 5.15 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907929, pre=0.910597, rec=0.905276\n",
      "Evaluation on dev at Epoch 89/100. Step:78142/87800: \n",
      "SpanFPreRecMetric: f=0.945387, pre=0.948349, rec=0.942444\n",
      "\n",
      "Evaluate data in 4.94 seconds!\n",
      "Evaluate data in 5.06 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908527, pre=0.909656, rec=0.907401\n",
      "Evaluation on dev at Epoch 90/100. Step:79020/87800: \n",
      "SpanFPreRecMetric: f=0.945264, pre=0.947422, rec=0.943117\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate data in 4.91 seconds!\n",
      "Evaluate data in 4.95 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907673, pre=0.908478, rec=0.90687\n",
      "Evaluation on dev at Epoch 91/100. Step:79898/87800: \n",
      "SpanFPreRecMetric: f=0.945433, pre=0.947591, rec=0.943285\n",
      "\n",
      "Evaluate data in 4.88 seconds!\n",
      "Evaluate data in 4.91 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908607, pre=0.909817, rec=0.907401\n",
      "Evaluation on dev at Epoch 92/100. Step:80776/87800: \n",
      "SpanFPreRecMetric: f=0.944829, pre=0.947226, rec=0.942444\n",
      "\n",
      "Evaluate data in 4.85 seconds!\n",
      "Evaluate data in 5.13 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907447, pre=0.908736, rec=0.906161\n",
      "Evaluation on dev at Epoch 93/100. Step:81654/87800: \n",
      "SpanFPreRecMetric: f=0.945504, pre=0.947903, rec=0.943117\n",
      "\n",
      "Evaluate data in 4.89 seconds!\n",
      "Evaluate data in 5.01 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908107, pre=0.909883, rec=0.906339\n",
      "Evaluation on dev at Epoch 94/100. Step:82532/87800: \n",
      "SpanFPreRecMetric: f=0.94593, pre=0.94825, rec=0.943622\n",
      "\n",
      "Evaluate data in 4.83 seconds!\n",
      "Evaluate data in 4.96 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.908736, pre=0.909542, rec=0.907932\n",
      "Evaluation on dev at Epoch 95/100. Step:83410/87800: \n",
      "SpanFPreRecMetric: f=0.944866, pre=0.946622, rec=0.943117\n",
      "\n",
      "Evaluate data in 4.92 seconds!\n",
      "Evaluate data in 4.96 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907705, pre=0.909075, rec=0.906339\n",
      "Evaluation on dev at Epoch 96/100. Step:84288/87800: \n",
      "SpanFPreRecMetric: f=0.946436, pre=0.948757, rec=0.944127\n",
      "\n",
      "Evaluate data in 4.76 seconds!\n",
      "Evaluate data in 4.91 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907609, pre=0.908172, rec=0.907047\n",
      "Evaluation on dev at Epoch 97/100. Step:85166/87800: \n",
      "SpanFPreRecMetric: f=0.945611, pre=0.947609, rec=0.943622\n",
      "\n",
      "Evaluate data in 5.07 seconds!\n",
      "Evaluate data in 4.9 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907655, pre=0.909511, rec=0.905807\n",
      "Evaluation on dev at Epoch 98/100. Step:86044/87800: \n",
      "SpanFPreRecMetric: f=0.945086, pre=0.947404, rec=0.94278\n",
      "\n",
      "Evaluate data in 4.92 seconds!\n",
      "Evaluate data in 4.93 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.90727, pre=0.908558, rec=0.905984\n",
      "Evaluation on dev at Epoch 99/100. Step:86922/87800: \n",
      "SpanFPreRecMetric: f=0.945096, pre=0.947253, rec=0.942949\n",
      "\n",
      "Evaluate data in 4.83 seconds!\n",
      "Evaluate data in 5.0 seconds!\n",
      "EvaluateCallback evaluation on data-test:\n",
      "SpanFPreRecMetric: f=0.907173, pre=0.908542, rec=0.905807\n",
      "Evaluation on dev at Epoch 100/100. Step:87800/87800: \n",
      "SpanFPreRecMetric: f=0.946019, pre=0.948258, rec=0.94379\n",
      "\n",
      "Best test performance(may not correspond to the best dev performance):{'SpanFPreRecMetric': {'f': 0.912113, 'pre': 0.910986, 'rec': 0.913244}} achieved at Epoch:73.\n",
      "Best test performance(correspond to the best dev performance):{'SpanFPreRecMetric': {'f': 0.909574, 'pre': 0.910866, 'rec': 0.908286}} achieved at Epoch:84.\n",
      "\n",
      "In Epoch:84/Step:73752, got best dev performance:\n",
      "SpanFPreRecMetric: f=0.946844, pre=0.949408, rec=0.944295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_eval': {'SpanFPreRecMetric': {'f': 0.946844,\n",
       "   'pre': 0.949408,\n",
       "   'rec': 0.944295}},\n",
       " 'best_epoch': 84,\n",
       " 'best_step': 73752,\n",
       " 'seconds': 26426.62}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fastNLP.embeddings import CNNCharEmbedding\n",
    "from fastNLP import cache_results\n",
    "from fastNLP import Trainer, GradientClipCallback, WarmupCallback\n",
    "from torch import optim\n",
    "from fastNLP import SpanFPreRecMetric, BucketSampler\n",
    "from fastNLP.io.pipe.conll import OntoNotesNERPipe\n",
    "from fastNLP.embeddings import StaticEmbedding, StackEmbedding, LSTMCharEmbedding\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "device = 0\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='en-ontonotes', choices=['conll2003', 'en-ontonotes'])\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "dataset = args.dataset\n",
    "\n",
    "if dataset == 'conll2003':\n",
    "    n_heads = 14\n",
    "    head_dims = 128\n",
    "    num_layers = 2\n",
    "    lr = 0.0009\n",
    "    attn_type = 'adatrans'\n",
    "    char_type = 'cnn'\n",
    "elif dataset == 'en-ontonotes':\n",
    "    n_heads =  8\n",
    "    head_dims = 96\n",
    "    num_layers = 2\n",
    "    lr = 0.0007\n",
    "    attn_type = 'adatrans'\n",
    "    char_type = 'adatrans'\n",
    "\n",
    "pos_embed = None\n",
    "\n",
    "#########hyper\n",
    "batch_size = 16\n",
    "warmup_steps = 0.01\n",
    "after_norm = 1\n",
    "model_type = 'transformer'\n",
    "normalize_embed = True\n",
    "#########hyper\n",
    "\n",
    "dropout=0.15\n",
    "fc_dropout=0.4\n",
    "\n",
    "encoding_type = 'bioes'\n",
    "name = 'caches/{}_{}_{}_{}_{}.pkl'.format(dataset, model_type, encoding_type, char_type, normalize_embed)\n",
    "d_model = n_heads * head_dims\n",
    "dim_feedforward = int(2 * d_model)\n",
    "\n",
    "\n",
    "\n",
    "@cache_results(name, _refresh=False)\n",
    "def load_data():\n",
    "    # 替换路径\n",
    "    #if dataset == 'conll2003':\n",
    "        # conll2003的lr不能超过0.002\n",
    "    paths = {'test': \"data/eng.testb.txt\",\n",
    "             'train':\"data/eng.train.txt\",\n",
    "             'dev': \"data/eng.testa.txt\"}\n",
    "    data = Conll2003NERPipe(encoding_type=encoding_type).process_from_file(paths)\n",
    "    #elif dataset == 'en-ontonotes':\n",
    "        # 会使用这个文件夹下的train.txt, test.txt, dev.txt等文件\n",
    "        #paths = '../data/en-ontonotes/english'\n",
    "        #data = OntoNotesNERPipe(encoding_type=encoding_type).process_from_file(paths)\n",
    "    char_embed = None\n",
    "    if char_type == 'cnn':\n",
    "        char_embed = CNNCharEmbedding(vocab=data.get_vocab('words'), embed_size=30, char_emb_size=30, filter_nums=[30],\n",
    "                                      kernel_sizes=[3], word_dropout=0, dropout=0.3, pool_method='max'\n",
    "                                      , include_word_start_end=False, min_char_freq=2)\n",
    "    elif char_type in ['adatrans', 'naive']:\n",
    "        char_embed = TransformerCharEmbed(vocab=data.get_vocab('words'), embed_size=30, char_emb_size=30, word_dropout=0,\n",
    "                 dropout=0.3, pool_method='max', activation='relu',\n",
    "                 min_char_freq=2, requires_grad=True, include_word_start_end=False,\n",
    "                 char_attn_type=char_type, char_n_head=3, char_dim_ffn=60, char_scale=char_type=='naive',\n",
    "                 char_dropout=0.15, char_after_norm=True)\n",
    "    elif char_type == 'lstm':\n",
    "        char_embed = LSTMCharEmbedding(vocab=data.get_vocab('words'), embed_size=30, char_emb_size=30, word_dropout=0,\n",
    "                 dropout=0.3, hidden_size=100, pool_method='max', activation='relu',\n",
    "                 min_char_freq=2, bidirectional=True, requires_grad=True, include_word_start_end=False)\n",
    "    word_embed = StaticEmbedding(vocab=data.get_vocab('words'),\n",
    "                                 model_dir_or_name='en-glove-6b-100d',\n",
    "                                 requires_grad=True, lower=True, word_dropout=0, dropout=0.5,\n",
    "                                 only_norm_found_vector=normalize_embed)\n",
    "    if char_embed is not None:\n",
    "        embed = StackEmbedding([word_embed, char_embed], dropout=0, word_dropout=0.02)\n",
    "    else:\n",
    "        word_embed.word_drop = 0.02\n",
    "        embed = word_embed\n",
    "\n",
    "    data.rename_field('words', 'chars')\n",
    "    return data, embed\n",
    "\n",
    "data_bundle, embed = load_data()\n",
    "print(data_bundle)\n",
    "\n",
    "model = TENER(tag_vocab=data_bundle.get_vocab('target'), embed=embed, num_layers=num_layers,\n",
    "                       d_model=d_model, n_head=n_heads,\n",
    "                       feedforward_dim=dim_feedforward, dropout=dropout,\n",
    "                        after_norm=after_norm, attn_type=attn_type,\n",
    "                       bi_embed=None,\n",
    "                        fc_dropout=fc_dropout,\n",
    "                       pos_embed=pos_embed,\n",
    "              scale=attn_type=='transformer')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "callbacks = []\n",
    "clip_callback = GradientClipCallback(clip_type='value', clip_value=5)\n",
    "evaluate_callback = EvaluateCallback(data_bundle.get_dataset('test'))\n",
    "\n",
    "if warmup_steps>0:\n",
    "    warmup_callback = WarmupCallback(warmup_steps, schedule='linear')\n",
    "    callbacks.append(warmup_callback)\n",
    "callbacks.extend([clip_callback, evaluate_callback])\n",
    "\n",
    "trainer = Trainer(data_bundle.get_dataset('train'), model, optimizer, batch_size=batch_size, sampler=BucketSampler(),\n",
    "                  num_workers=0, n_epochs=100, dev_data=data_bundle.get_dataset('dev'),\n",
    "                  metrics=SpanFPreRecMetric(tag_vocab=data_bundle.get_vocab('target'), encoding_type=encoding_type),\n",
    "                  dev_batch_size=batch_size*5, callbacks=callbacks, device=device, test_use_tqdm=False,\n",
    "                  use_tqdm=True, print_every=300, save_path=None)\n",
    "\n",
    "trainer.train(load_best_model=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
